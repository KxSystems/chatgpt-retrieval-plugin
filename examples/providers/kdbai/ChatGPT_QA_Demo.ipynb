{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDB.AI for Q&A with ChatGPT Retrieval Plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will demonstrate how to use the ChatGPT Retrieval Plugin to embed and store data in KDB.AI, as well as to query the embedded data. The plugin enables ChatGPT large language models, such as GPT-3.5, to be used for querying data that it wasn't trained on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the ChatGPT Retrieval Plugin work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kdbai-chatgpt-overview](kdbai-chatgpt-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram above provides a high-level overview of the system, and explains how ChatGPT, the Retrieval Plugin, and KDB.AI all interact with one another. The system can be broken down into two main sub-systems: upserting (shown in red) and querying (shown in blue).\n",
    "\n",
    "\n",
    "### Upserting\n",
    "\n",
    "Upserting is the process of taking our own dataset and uploading it to our vector database. In order to upsert data, we must pre-process it to remove any part we do not want to store in the database, change the format to a dictionary, and divide it into batches. When we call  `/upsert`, each batch of data is embedded using an OpenAI LLM (Large Language Model), to generate a vector embedding for similarity search. The data and its embedding are then inserted into a KDB.AI table.\n",
    "\n",
    "\n",
    "### Querying\n",
    "\n",
    "Querying involves taking an input query, or question, transforming it into an embedded vector, and running similarity search in the vector database to find its nearest neighbours - data with closest relevancy to the query. When we call `/query`,  the input query is embedded with the same OpenAI LLM used for upserting, to generate a query vector. A similarity search algorithm running within KDB.AI will return the N closest matches with highest relevancy to the query. This \"context\" is then sent back to ChatGPT via the Retrieval Plugin. Finally, using this contextual information, ChatGPT outputs a human-like response to the query.\n",
    "\n",
    "Having an external source of data greatly increases the applicability of ChatGPT to different tasks. For example: GPT-3.5 can only answer questions with information it has seen during training, which is capped at 2021.. Therefore, if you asked it who the 2023 UK Prime Minister is, the model does not have the necessary information. However, if you were to download a dataset of political data up to the present day, ChatGPT would be able to use this data to answer the question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3\n",
    "- Pip\n",
    "- Git\n",
    "\n",
    "### Install the KDB.AI ChatGPT Retrieval Plugin server app\n",
    "\n",
    "```\n",
    "git clone https://github.com/KxSystems/chatgpt-retrieval-plugin -b KDB.AI\n",
    "cd chatgpt-retrieval-plugin\n",
    "pip install poetry\n",
    "poetry install\n",
    "```\n",
    "\n",
    "### Run the KDB.AI ChatGPT Retrieval Plugin server app\n",
    "\n",
    "```\n",
    "export BEARER_TOKEN='<BEARER TOKEN>'  # you can create your own bearer token on auth0.com\n",
    "export DATASTORE=kdbai\n",
    "export KDBAI_ENDPOINT='<KDB.AI ENDPOINT>'\n",
    "export KDBAI_API_KEY='<KDB.AI API KEY>'\n",
    "export OPENAI_API_KEY='<OPENAI API KEY>'  # You can get a free API key on https://platform.openai.com\n",
    "\n",
    "poetry run start\n",
    "```\n",
    "\n",
    "### Install a separate Jupyter environment to run this notebook\n",
    "\n",
    "```\n",
    "pip install datasets jupyter openai tqdm\n",
    "```\n",
    "\n",
    "### Run Jupyter\n",
    "\n",
    "```\n",
    "export BEARER_TOKEN='<BEARER TOKEN>'  # Same bearer token as above\n",
    "export OPENAI_API_KEY='<OPENAI API KEY>'\n",
    "\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Then open this notebook in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "from datasets import load_dataset\n",
    "import openai\n",
    "import requests\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEARER_TOKEN = os.environ.get(\"BEARER_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset from Hugging Face\n",
    "\n",
    "The Adversarial_QA dataset is chosen for this demonstration. It consists of questions that current state-of-the-art models find challenging, paired with contextual data that can be used to formulate an answer. Some of these questions have very poor grammar (\"What sare the benifts of the blood brain barrir?\"), and others are purposefully vague (\"What is at the highest level?\"), as shown in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"adversarial_qa\", 'adversarialQA', split=\"train\").to_pandas()\n",
    "data = data.drop_duplicates(subset=[\"context\"])\n",
    "print(f\"Number of unique contexts: {len(data)}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this dataset for Q&A with ChatGPT, we need to insert the relevant data into our vector datastore - KDB.AI. The only column of data we will insert is the \"context\" column, which we reformat into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text data from the dataset\n",
    "documents = [\n",
    "    {\n",
    "        'text': r['context'],\n",
    "    } for r in data.to_dict(orient='records')\n",
    "]\n",
    "pprint(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data to the KDB.AI table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise an HTTP session with the KDB.AI ChatGPT Retrieval Plugin app\n",
    "s = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `/upsert` instruction is used to insert data to the KDB.AI datastore in batches, with each batch being embedded with OpenAI Embedding Model `text-embedding-ada-002` before it is added to the table. We take our contextual data, which has been reformatted into a dictionary, and call `/upsert` on batches of 100 documents at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 100\n",
    "\n",
    "# upsert documents from the dataset in batches\n",
    "for i in tqdm(range(0, len(documents), batchSize)):\n",
    "    i_end = min(len(documents), i+batchSize)\n",
    "    \n",
    "    res = s.post(\n",
    "        \"http://localhost:8000/upsert\",\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {BEARER_TOKEN}\"\n",
    "        },\n",
    "        \n",
    "        json = {\n",
    "            \"documents\": documents[i:i_end]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the KDB.AI table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the list of questions from our dataset and format them into a dictionary. We will randomly choose 5 of these questions to be our queries in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract questions and reformat into queries\n",
    "queries = data['question'].tolist()\n",
    "queries = [{'query': queries[i]} for i in range(len(queries))]\n",
    "\n",
    "# choose 5 queries at random \n",
    "i = random.randint(0, len(queries)-5)\n",
    "searchQueries = queries[i:i+5]\n",
    "\n",
    "print(searchQueries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `/query` instruction is used to extract relevant information from the KDB.AI datastore. The queries are embedded into vectors, and a similarity search algorithm is used to calculate its nearest neighbours, representing the most relevant entries in the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the vector database\n",
    "results = requests.post(\n",
    "    \"http://localhost:8000/query\",\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {BEARER_TOKEN}\"\n",
    "    },\n",
    "    \n",
    "    json = {\n",
    "        'queries': searchQueries\n",
    "    }\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we iterate through each query and its results, and pass this data to ChatGPT, which uses it to respond to the query with natural language. Here, you can see the three \"nearest neighbour\" pieces of context that ChatGPT uses to form its answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each set of queries/results\n",
    "for query_response in results.json()['results']:\n",
    "    query = query_response['query']\n",
    "    answers = []\n",
    "    scores = []\n",
    "    \n",
    "    # extract answers and scores from each result\n",
    "    for result in query_response['results']:\n",
    "        \n",
    "        # answer = textual information related to the query\n",
    "        answers.append(result['text'])\n",
    "        \n",
    "        # score = distance between the query vector and the answer vector (smaller=better!)\n",
    "        scores.append(round(result['score'], 2))\n",
    "    \n",
    "    # print the query\n",
    "    print(\"\\nQUERY:\\n\"+query)\n",
    "    \n",
    "    # print the query responses, and their scores\n",
    "    print(\"\\nCONTEXT:\\n\"+\"\\n\".join([f\"{s}: {a}\" for a, s in zip(answers, scores)])+\"\\n\")\n",
    "    \n",
    "    # format the query and its answers into GPT messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"You are a helpful assistant with the following knowledge: {answers}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Using your knowledge, answer this query: {query}\"}\n",
    "    ]\n",
    "    \n",
    "    # send the messages to a GPT model\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=messages,\n",
    "      max_tokens=100,\n",
    "      n=1,\n",
    "      stop=None,            \n",
    "    )\n",
    "\n",
    "    # extract the generated response from the API response\n",
    "    generated_response =response['choices'][0]['message']['content']\n",
    "    \n",
    "    # Print the generated response\n",
    "    print(f\"RESPONSE: \\n{generated_response}\\n\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the KDB.AI table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the KDB.AI table, run the code cell below. Once this is done, the ChatGPT Retrieval Plugin app will need to be restarted to create another table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One would need to restart the KDB.AI ChatGPT Retrieval Plugin server app\n",
    "## after this, to recreate the table\n",
    "res = requests.delete(\n",
    "    \"http://localhost:8000/delete\",\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {BEARER_TOKEN}\"\n",
    "    }, \n",
    "    json = {\n",
    "        \"delete_all\": True\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
